{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# create ShapTime\n",
    "def get_sub_set(Tn):\n",
    "    \n",
    "    mylist = list(range(Tn))\n",
    "    sub_sets = [[]]\n",
    "    for x in mylist:\n",
    "        sub_sets.extend([item + [x] for item in sub_sets])\n",
    "    return sub_sets \n",
    "\n",
    "\n",
    "def ValFunction(model, interp_x, Tn):\n",
    "    \n",
    "    dfx = supertime(interp_x)\n",
    "    # Generate subsets of indexes\n",
    "    subset = get_sub_set(Tn)   \n",
    "    \n",
    "    # Generate the baseline\n",
    "    y_results = model.predict(interp_x)\n",
    "    baseline = sum(y_results)/len(interp_x)\n",
    "    \n",
    "    val_results = []\n",
    "    \n",
    "    for i in range(1, len(subset)):\n",
    "        x_i = dfx[subset[i][0]]\n",
    "        \n",
    "        if len(subset[i]) == 1:\n",
    "            prediction = model.predict(x_i)\n",
    "            results = (sum(prediction)/len(x_i)) - baseline\n",
    "            val_results.append(results)\n",
    "            \n",
    "        else:\n",
    "            for n in range(1, len(subset[i])):\n",
    "                x_i = np.concatenate([x_i, dfx[subset[i][n]]], axis = 0)\n",
    "            \n",
    "            prediction = model.predict(x_i)\n",
    "            results = (sum(prediction)/len(x_i)) - baseline\n",
    "            val_results.append(results)\n",
    "            \n",
    "    val_results.insert(0,0.0)\n",
    "            \n",
    "    return subset, val_results\n",
    "\n",
    "\n",
    "def index(Si, subset):\n",
    "    for i in range(len(subset)):\n",
    "        if Si == subset[i]:\n",
    "            index = i\n",
    "        else:\n",
    "            pass\n",
    "    return index\n",
    "\n",
    "\n",
    "def ShapleyValues(model, interp_x, Tn):\n",
    "    \n",
    "    subset, val_results = ValFunction(model, interp_x, Tn)\n",
    "    shapley_values = []\n",
    "    for i in range (Tn):\n",
    "        shapley = []\n",
    "        for n in range(len(subset)):\n",
    "            if i not in subset[n]:\n",
    "                Si = subset[n]+[i]\n",
    "                Si.sort()\n",
    "                Si_num = index(Si, subset)\n",
    "            \n",
    "                S_num = len(subset[n])\n",
    "                N = Tn\n",
    "            \n",
    "                weight = (math.factorial(S_num) * math.factorial((N-S_num-1))) / math.factorial(N)\n",
    "                val = val_results[Si_num] - val_results[n]\n",
    "                shapley_i = weight * val\n",
    "            \n",
    "                shapley.append(shapley_i)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        shapley_values.append(sum(shapley))\n",
    "        del shapley\n",
    "        \n",
    "    return shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapley values for each time step:\n",
      "Time step 1: [[-2.71563378e-03 -9.67365538e-04  1.29624848e-02 -5.39228363e-03\n",
      "   4.96418171e-03 -4.89989192e-03 -1.45903771e-02 -1.80328239e-03]\n",
      " [ 3.91408133e-03 -2.72475193e-03  2.88413164e-03 -1.55050342e-03\n",
      "   2.51221023e-03  3.15951254e-03  3.40966478e-03  2.54396347e-03]\n",
      " [ 2.34615759e-03  3.78188432e-03  1.86391219e-03 -9.80290463e-03\n",
      "   1.16872917e-02 -6.89484202e-03 -2.09789490e-03 -3.90287245e-03]\n",
      " [-2.36641499e-03 -7.26922271e-04  2.50059202e-03  1.63412332e-03\n",
      "   4.14464570e-03 -3.12541536e-03 -4.50040583e-05 -1.37131191e-03]]\n",
      "Time step 2: [[-5.35084032e-05 -5.15243762e-03  7.30762494e-03  3.59459254e-03\n",
      "   9.72397493e-03 -3.32050520e-03 -7.58557375e-03 -2.33392430e-03]\n",
      " [ 1.15056279e-02  1.15544161e-03  4.17861521e-04  4.49670953e-03\n",
      "   1.62694384e-03  4.14239204e-03  7.05176699e-03 -3.60923063e-03]\n",
      " [ 4.55067022e-03  8.02245024e-03 -1.26170878e-03 -1.75571746e-03\n",
      "   5.75336818e-03 -2.69810456e-03 -3.95257481e-04  3.93005168e-04]\n",
      " [-1.19119498e-03 -8.63462588e-03  2.26734369e-04  1.19209050e-03\n",
      "   2.82295901e-03 -5.53345033e-04 -1.93674542e-03  6.97029972e-03]]\n",
      "Time step 3: [[-5.41176118e-03 -4.31549460e-03  5.46210373e-03 -3.76971653e-03\n",
      "   6.09931404e-03  3.24489451e-04 -3.76651565e-03  3.21768099e-03]\n",
      " [ 6.19287701e-03 -4.74001969e-04  4.91981736e-05  1.18802904e-03\n",
      "  -3.70954743e-03  4.90673965e-03  5.66952201e-03  5.10893548e-03]\n",
      " [ 1.43365603e-03  8.02918645e-03 -7.03334757e-03  2.84408982e-03\n",
      "   9.07931444e-03 -1.28931830e-03  1.82418136e-03 -2.32423133e-04]\n",
      " [-1.31032302e-03 -9.47329242e-03  4.19898876e-03  6.42239657e-03\n",
      "   1.53299525e-03 -2.05019291e-03 -6.25591409e-03 -4.68533932e-03]]\n",
      "Time step 4: [[ 0.00377719  0.00485011  0.01404476 -0.00918057  0.00908752 -0.00411291\n",
      "  -0.00729512  0.00316033]\n",
      " [ 0.00653714  0.00452177  0.00869167  0.00202683  0.00060822  0.00245225\n",
      "   0.00504597 -0.00235876]\n",
      " [-0.00140159  0.00530071  0.00222317 -0.006206    0.00916565  0.00014201\n",
      "  -0.00433559 -0.00057123]\n",
      " [-0.01014427 -0.00897299 -0.00160646  0.00174134 -0.00914625 -0.00486642\n",
      "  -0.00034307  0.00408073]]\n",
      "Time step 5: [[-0.00440522  0.00188939  0.01134734 -0.00470831  0.00324606 -0.00619314\n",
      "  -0.0042939  -0.00219404]\n",
      " [ 0.00679846  0.0005697  -0.00213217 -0.00471424 -0.00140023 -0.00043368\n",
      "   0.0042487   0.00160683]\n",
      " [ 0.00891953  0.00748029  0.01173463  0.00179419  0.01434386 -0.00520668\n",
      "  -0.00094474 -0.00297036]\n",
      " [-0.00506724  0.0012682   0.00490166 -0.00284014 -0.00121881 -0.00395434\n",
      "   0.00283332  0.00333664]]\n",
      "Time step 6: [[-6.55080304e-03 -2.43667965e-03  5.18511078e-03 -3.05669656e-03\n",
      "   5.38236664e-03  5.92720106e-04 -1.31798504e-02  2.22587233e-03]\n",
      " [ 1.04311106e-02  1.98464671e-04  9.17379975e-05 -2.59765536e-03\n",
      "  -1.62142205e-03 -2.86335098e-03 -1.56709975e-03  4.00053055e-04]\n",
      " [ 7.94810828e-03  7.99252673e-03  6.83482228e-03  4.31124476e-03\n",
      "   8.05182366e-03  1.48444799e-03  1.37496403e-03  1.34715066e-03]\n",
      " [-4.99797901e-03  3.58963542e-04  2.21540957e-04  1.10189883e-03\n",
      "  -4.70625494e-04 -7.46366478e-03 -3.47391750e-04  1.50541144e-03]]\n",
      "Time step 7: [[-0.0012667  -0.00293442  0.01161589 -0.00376509  0.00070566  0.00421477\n",
      "  -0.00440261  0.00318893]\n",
      " [ 0.01276157 -0.00278933  0.00490257  0.00045298  0.00025937 -0.0021179\n",
      "   0.0017197   0.00285136]\n",
      " [ 0.00247061  0.00770351  0.00642633  0.00108685  0.01523741  0.00023559\n",
      "  -0.00542786  0.00029933]\n",
      " [ 0.0002938   0.00213171  0.00495551 -0.00185846 -0.00029585 -0.00274983\n",
      "  -0.00285098  0.00649723]]\n",
      "Time step 8: [[ 4.94728775e-03 -1.29666108e-03  4.86312553e-03 -6.38059481e-03\n",
      "   5.05300711e-03 -6.12015252e-03 -8.39627401e-03  9.11845612e-04]\n",
      " [ 1.03660333e-02 -2.57124378e-03  1.35714189e-03 -1.35314396e-03\n",
      "  -6.22241844e-04  4.10956127e-03  3.53434062e-03 -2.07113455e-03]\n",
      " [ 1.14404268e-03  1.00503910e-02 -5.17886912e-03  8.26554438e-05\n",
      "   9.34974311e-03 -1.72493230e-03 -2.52325474e-03 -4.75631263e-03]\n",
      " [-2.61339290e-03 -1.85688997e-03  7.09234487e-04  1.06288202e-03\n",
      "  -4.52825909e-04 -6.14757922e-03  4.26716947e-03  5.71582599e-03]]\n",
      "Time step 9: [[ 5.28187780e-03 -6.65033271e-04  7.14430180e-03 -6.29990202e-03\n",
      "   7.67531546e-03 -5.48784221e-03  2.09649180e-03 -3.62477575e-03]\n",
      " [ 7.90996836e-04 -7.39901848e-03 -1.55038551e-03  3.59246632e-04\n",
      "   4.97973225e-04  3.13771669e-03  2.36912736e-03  9.31089993e-05]\n",
      " [ 4.32452395e-03  1.42887685e-03  1.22286850e-03 -1.25346413e-03\n",
      "   9.82634554e-03 -7.00357812e-03  2.05150351e-03 -4.07009474e-03]\n",
      " [-7.37029602e-03 -3.12151198e-03 -3.07619970e-03 -1.75275709e-03\n",
      "   3.91556668e-03 -4.76996063e-03 -1.80237084e-03  1.03747516e-02]]\n",
      "Time step 10: [[-4.66172085e-03 -3.65353583e-03  1.14779529e-02 -6.28028420e-03\n",
      "   3.46955409e-03 -6.46036207e-03 -5.32742637e-03 -3.33071496e-03]\n",
      " [ 1.02458672e-02  3.98962414e-04  6.26158906e-04 -4.86429894e-03\n",
      "   3.07449222e-03  7.81606488e-05  5.07823354e-03  3.67847306e-03]\n",
      " [-1.90748490e-03  2.95137561e-03 -1.70445530e-03 -1.07966738e-03\n",
      "   9.71953600e-03  8.76244666e-05 -6.88520172e-03 -4.52604065e-03]\n",
      " [ 1.27670819e-03 -2.80683693e-03 -5.65863640e-04  1.31558558e-03\n",
      "   4.40010964e-03 -3.97825490e-03  6.63768105e-04  4.94295399e-03]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X_train = np.random.rand(100, 10, 8)  # Input data: 100 samples, 10 time steps, 8 features\n",
    "y_train = np.random.rand(100, 4, 8)   # Output data: 100 samples, 4 time steps, 8 features\n",
    "\n",
    "\n",
    "# Define a black-box model that takes 3D input and produces 3D output\n",
    "class ExampleModel:\n",
    "    def predict(self, X):\n",
    "        return np.random.rand(X.shape[0], 4, 8)\n",
    "# Initialize the black-box model\n",
    "model = ExampleModel()\n",
    "\n",
    "\n",
    "# Define the number of time steps\n",
    "Tn = 10\n",
    "\n",
    "# Function to simulate supertime\n",
    "def supertime(interp_x):\n",
    "    dfx = []\n",
    "    for i in range(interp_x.shape[1]):  # Iterate over the second axis (time steps)\n",
    "        dfx.append(interp_x[:, i, :])  # Select all samples at the current time step\n",
    "    return dfx\n",
    "\n",
    "\n",
    "\n",
    "# Compute Shapley values\n",
    "shapley_values = ShapleyValues(model, X_train, Tn)\n",
    "\n",
    "\n",
    "# Print Shapley values for each time step\n",
    "print(\"Shapley values for each time step:\")\n",
    "for i, shapley_value in enumerate(shapley_values):\n",
    "    print(f\"Time step {i+1}: {shapley_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapley_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00466172, -0.00365354,  0.01147795, -0.00628028,  0.00346955,\n",
       "       -0.00646036, -0.00532743, -0.00333071])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapley_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 02:01:22.668728: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-28 02:01:22.696141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-28 02:01:22.696164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-28 02:01:22.696872: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-28 02:01:22.701530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 02:01:23.244463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Simulate weather data (wind speed, temperature, humidity, precipitation)\n",
    "np.random.seed(0)\n",
    "num_samples = 1000\n",
    "num_features = 4\n",
    "time_steps = 10  # Historical time steps\n",
    "forecast_steps = 4  # Forecasting steps\n",
    "\n",
    "wind_speed = np.random.rand(num_samples, time_steps)\n",
    "temperature = np.random.rand(num_samples, time_steps)\n",
    "humidity = np.random.rand(num_samples, time_steps)\n",
    "precipitation = np.random.rand(num_samples, time_steps)\n",
    "\n",
    "# Combine features into a DataFrame\n",
    "weather_data = pd.DataFrame({\n",
    "    'Wind Speed': wind_speed.ravel(),\n",
    "    'Temperature': temperature.ravel(),\n",
    "    'Humidity': humidity.ravel(),\n",
    "    'Precipitation': precipitation.ravel()\n",
    "})\n",
    "\n",
    "# Simulate target (wind speed and temperature forecast)\n",
    "target_wind_speed = np.random.rand(num_samples, forecast_steps)\n",
    "target_temperature = np.random.rand(num_samples, forecast_steps)\n",
    "\n",
    "# Combine targets into a DataFrame\n",
    "forecast_data = pd.DataFrame({\n",
    "    'Target Wind Speed': target_wind_speed.ravel(),\n",
    "    'Target Temperature': target_temperature.ravel()\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Target Wind Speed</th>\n",
       "      <th>Target Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.748268</td>\n",
       "      <td>0.392173</td>\n",
       "      <td>0.758125</td>\n",
       "      <td>0.369256</td>\n",
       "      <td>0.726851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.180203</td>\n",
       "      <td>0.041157</td>\n",
       "      <td>0.503319</td>\n",
       "      <td>0.211326</td>\n",
       "      <td>0.906916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.389023</td>\n",
       "      <td>0.923301</td>\n",
       "      <td>0.177017</td>\n",
       "      <td>0.476905</td>\n",
       "      <td>0.383102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.406235</td>\n",
       "      <td>0.832537</td>\n",
       "      <td>0.082234</td>\n",
       "      <td>0.221634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.011788</td>\n",
       "      <td>0.944282</td>\n",
       "      <td>0.516825</td>\n",
       "      <td>0.237659</td>\n",
       "      <td>0.046115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.550447</td>\n",
       "      <td>0.748175</td>\n",
       "      <td>0.622359</td>\n",
       "      <td>0.668383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.397151</td>\n",
       "      <td>0.298267</td>\n",
       "      <td>0.531140</td>\n",
       "      <td>0.994911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.758430</td>\n",
       "      <td>0.446456</td>\n",
       "      <td>0.852904</td>\n",
       "      <td>0.588224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.023787</td>\n",
       "      <td>0.360127</td>\n",
       "      <td>0.734230</td>\n",
       "      <td>0.424615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.813575</td>\n",
       "      <td>0.625887</td>\n",
       "      <td>0.638997</td>\n",
       "      <td>0.901633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Wind Speed  Temperature  Humidity  Precipitation  Target Wind Speed  \\\n",
       "0       0.548814     0.748268  0.392173       0.758125           0.369256   \n",
       "1       0.715189     0.180203  0.041157       0.503319           0.211326   \n",
       "2       0.602763     0.389023  0.923301       0.177017           0.476905   \n",
       "3       0.544883     0.037600  0.406235       0.832537           0.082234   \n",
       "4       0.423655     0.011788  0.944282       0.516825           0.237659   \n",
       "...          ...          ...       ...            ...                ...   \n",
       "9995    0.550447     0.748175  0.622359       0.668383                NaN   \n",
       "9996    0.397151     0.298267  0.531140       0.994911                NaN   \n",
       "9997    0.758430     0.446456  0.852904       0.588224                NaN   \n",
       "9998    0.023787     0.360127  0.734230       0.424615                NaN   \n",
       "9999    0.813575     0.625887  0.638997       0.901633                NaN   \n",
       "\n",
       "      Target Temperature  \n",
       "0               0.726851  \n",
       "1               0.906916  \n",
       "2               0.383102  \n",
       "3               0.221634  \n",
       "4               0.046115  \n",
       "...                  ...  \n",
       "9995                 NaN  \n",
       "9996                 NaN  \n",
       "9997                 NaN  \n",
       "9998                 NaN  \n",
       "9999                 NaN  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate weather data and forecast data\n",
    "full_data = pd.concat([weather_data, forecast_data], axis=1)\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = full_data.drop(['Target Wind Speed', 'Target Temperature'], axis=1).values.reshape(-1, time_steps, num_features)\n",
    "y = full_data[['Target Wind Speed', 'Target Temperature']].values.reshape(-1, forecast_steps, 2)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the Shapley value computation functions\n",
    "\n",
    "def get_sub_set(Tn):\n",
    "    mylist = list(range(Tn))\n",
    "    sub_sets = [[]]\n",
    "    for x in mylist:\n",
    "        sub_sets.extend([item + [x] for item in sub_sets])\n",
    "    return sub_sets \n",
    "\n",
    "def supertime(interp_x):\n",
    "    dfx = []\n",
    "    for i in range(interp_x.shape[1]):  # Iterate over the second axis (time steps)\n",
    "        dfx.append(interp_x[:, i, :])  # Select all samples at the current time step\n",
    "    return dfx\n",
    "\n",
    "def ValFunction(model, interp_x, Tn):\n",
    "    dfx = supertime(interp_x)\n",
    "    # Generate subsets of indexes\n",
    "    subset = get_sub_set(Tn)   \n",
    "    \n",
    "    # Generate the baseline\n",
    "    y_results = model.predict(interp_x)\n",
    "    baseline = np.mean(y_results, axis=0)\n",
    "    \n",
    "    val_results = []\n",
    "    \n",
    "    for i in range(1, len(subset)):\n",
    "        x_i = dfx[subset[i][0]]\n",
    "        \n",
    "        if len(subset[i]) == 1:\n",
    "            prediction = model.predict(x_i)\n",
    "            results = (np.mean(prediction, axis=0)) - baseline\n",
    "            val_results.append(results)\n",
    "            \n",
    "        else:\n",
    "            for n in range(1, len(subset[i])):\n",
    "                x_i = np.concatenate([x_i, dfx[subset[i][n]]], axis=0)\n",
    "            \n",
    "            prediction = model.predict(x_i)\n",
    "            results = (np.mean(prediction, axis=0)) - baseline\n",
    "            val_results.append(results)\n",
    "            \n",
    "    val_results.insert(0, np.zeros(baseline.shape))\n",
    "            \n",
    "    return subset, val_results\n",
    "\n",
    "def index(Si, subset):\n",
    "    for i in range(len(subset)):\n",
    "        if Si == subset[i]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def ShapleyValues(model, interp_x, Tn):\n",
    "    subset, val_results = ValFunction(model, interp_x, Tn)\n",
    "    shapley_values = []\n",
    "    for i in range(Tn):\n",
    "        shapley = []\n",
    "        for n in range(len(subset)):\n",
    "            if i not in subset[n]:\n",
    "                Si = subset[n]+[i]\n",
    "                Si.sort()\n",
    "                Si_num = index(Si, subset)\n",
    "                S_num = len(subset[n])\n",
    "                N = Tn\n",
    "                weight = (math.factorial(S_num) * math.factorial((N-S_num-1))) / math.factorial(N)\n",
    "                val = val_results[Si_num] - val_results[n]\n",
    "                shapley_i = weight * val\n",
    "                shapley.append(shapley_i)\n",
    "        shapley_values.append(np.sum(shapley, axis=0))\n",
    "    return shapley_values\n",
    "\n",
    "# Create a simple LSTM model for forecasting\n",
    "def create_lstm_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape, return_sequences=True),\n",
    "        LSTM(64),\n",
    "        Dense(output_shape)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, num_features)).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, num_features)).reshape(X_test.shape)\n",
    "\n",
    "# Define input shape and output shape for the LSTM model\n",
    "input_shape = (time_steps, num_features)\n",
    "output_shape = forecast_steps * 2  # 2 targets: wind speed and temperature\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "model = create_lstm_model(input_shape, output_shape)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Compute Shapley values\n",
    "shapley_values = ShapleyValues(model, X_test_scaled, time_steps)\n",
    "\n",
    "# Print Shapley values for each time step\n",
    "print(\"Shapley values for each time step:\")\n",
    "for i, shapley_val in enumerate(shapley_values):\n",
    "    print(f\"Time Step {i + 1}: {shapley_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Shapley value computation functions\n",
    "\n",
    "def get_sub_set(Tn):\n",
    "    mylist = list(range(Tn))\n",
    "    sub_sets = [[]]\n",
    "    for x in mylist:\n",
    "        sub_sets.extend([item + [x] for item in sub_sets])\n",
    "    return sub_sets \n",
    "\n",
    "def supertime(interp_x):\n",
    "    dfx = []\n",
    "    for i in range(interp_x.shape[1]):  # Iterate over the second axis (time steps)\n",
    "        dfx.append(interp_x[:, i, :])  # Select all samples at the current time step\n",
    "    return dfx\n",
    "\n",
    "def ValFunction(model, interp_x, Tn):\n",
    "    dfx = supertime(interp_x)\n",
    "    # Generate subsets of indexes\n",
    "    subset = get_sub_set(Tn)   \n",
    "    \n",
    "    # Generate the baseline\n",
    "    y_results = model.predict(interp_x)\n",
    "    baseline = np.mean(y_results, axis=0)\n",
    "    \n",
    "    val_results = []\n",
    "    \n",
    "    for i in range(1, len(subset)):\n",
    "        x_i = dfx[subset[i][0]]\n",
    "        \n",
    "        if len(subset[i]) == 1:\n",
    "            print(x_i)\n",
    "            myBlackbox = BlackBoxWrapper(model) ## Ajout\n",
    "            #prediction = model.predict(x_i)\n",
    "            prediction = myBlackbox.predict(x_i)\n",
    "            print(prediction)\n",
    "            results = (np.mean(prediction, axis=0)) - baseline\n",
    "            val_results.append(results)\n",
    "            \n",
    "        else:\n",
    "            for n in range(1, len(subset[i])):\n",
    "                x_i = np.concatenate([x_i, dfx[subset[i][n]]], axis=0)\n",
    "            \n",
    "            #prediction = model.predict(x_i)\n",
    "            prediction = myBlackbox.predict(x_i)\n",
    "            results = (np.mean(prediction, axis=0)) - baseline\n",
    "            val_results.append(results)\n",
    "            \n",
    "    val_results.insert(0, np.zeros(baseline.shape))\n",
    "            \n",
    "    return subset, val_results\n",
    "\n",
    "def index(Si, subset):\n",
    "    for i in range(len(subset)):\n",
    "        if Si == subset[i]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def ShapleyValues(model, interp_x, Tn):\n",
    "    subset, val_results = ValFunction(model, interp_x, Tn)\n",
    "    shapley_values = []\n",
    "    for i in range(Tn):\n",
    "        shapley = []\n",
    "        for n in range(len(subset)):\n",
    "            if i not in subset[n]:\n",
    "                Si = subset[n]+[i]\n",
    "                Si.sort()\n",
    "                Si_num = index(Si, subset)\n",
    "                S_num = len(subset[n])\n",
    "                N = Tn\n",
    "                weight = (math.factorial(S_num) * math.factorial((N-S_num-1))) / math.factorial(N)\n",
    "                val = val_results[Si_num] - val_results[n]\n",
    "                shapley_i = weight * val\n",
    "                shapley.append(shapley_i)\n",
    "        shapley_values.append(np.sum(shapley, axis=0))\n",
    "    return shapley_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one prediction shapley value! make it recursive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "SHAP values: [-0.47171305  0.26673904  0.01888289  0.60529344  0.07295896 -0.11923268\n",
      " -0.20718638  0.14425109]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(\n",
    "        itertools.combinations(s, r) for r in range(len(s) + 1))\n",
    "\n",
    "def shapley_kernel(M, s):\n",
    "    if s == 0 or s == M:\n",
    "        return 10000\n",
    "    return (M - 1) / (scipy.special.binom(M, s) * s * (M - s))\n",
    "\n",
    "# Adjusted f function as needed to match your specific forecasting model's input/output structure\n",
    "def f(X):\n",
    "    # Example placeholder function\n",
    "    np.random.seed(0)\n",
    "    beta = np.random.rand(X.shape[-1])  # Assuming a flattened structure for X\n",
    "    return np.dot(X, beta) + 10\n",
    "\n",
    "def kernel_shap(f, x, reference, M):\n",
    "    n_timesteps, n_features = x.shape\n",
    "    X = np.zeros((2**M, M + 1))\n",
    "    X[:, -1] = 1  # Bias term\n",
    "    weights = np.zeros(2**M)\n",
    "    V = np.zeros((2**M, n_timesteps, n_features))\n",
    "    print(V)\n",
    "    \n",
    "    for i in range(2**M):\n",
    "        V[i, :, :] = reference  # Start with reference values\n",
    "\n",
    "    for i, s in enumerate(powerset(range(M))):\n",
    "        s = list(s)\n",
    "        if s:\n",
    "            # Adjust assignment for broadcasting\n",
    "            for feature_index in s:\n",
    "                V[i, :, feature_index] = x[:, feature_index]\n",
    "        X[i, s] = 1\n",
    "        weights[i] = shapley_kernel(M, len(s))\n",
    "\n",
    "    y = f(V.reshape(-1, n_timesteps * n_features)).reshape(-1, n_timesteps)  # Ensure f's expected input shape\n",
    "    weights_sqrt = np.sqrt(weights)\n",
    "    results = np.linalg.lstsq(weights_sqrt[:, None] * X, weights_sqrt * y.ravel(), rcond=None)[0]\n",
    "    return results[:-1]  # Exclude bias term from results\n",
    "\n",
    "# Example usage\n",
    "M = 8  # Number of features to be considered in the powerset\n",
    "n_timesteps = 2\n",
    "n_features = M\n",
    "x = np.random.rand(n_timesteps, n_features)  # Sample input\n",
    "reference = np.random.rand(n_timesteps, n_features)  # Reference values\n",
    "\n",
    "results = kernel_shap(f, x, reference, M)\n",
    "print(\"SHAP values:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43758721, 0.891773  , 0.96366276],\n",
       "       [0.38344152, 0.79172504, 0.52889492]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56804456, 0.92559664, 0.07103606],\n",
       "       [0.0871293 , 0.0202184 , 0.83261985]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    np.random.seed(0)\n",
    "    beta = np.random.rand(X.shape[-1])\n",
    "    return np.dot(X, beta) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_adaptive(model, y):\n",
    "    n_features = y.shape[1]\n",
    "    shapley_values = np.zeros(n_features)\n",
    "    n_samples = x.shape[0]\n",
    "\n",
    "    # step 1: Define the feature's coalition\n",
    "    coalition = list(itertools.chain.from_iterable(itertools.combinations(range(n_features), r) for r in range(n_features + 1)))\n",
    "    \n",
    "    # Compute marginal contributions without feature i\n",
    "    for i in range(1):\n",
    "        for subset in coalition:\n",
    "            # without feature i\n",
    "            print(subset)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(10, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalition = list(itertools.chain.from_iterable(itertools.combinations(range(8), r) for r in range(8 + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coalition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalition[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763,\n",
       "        -2.3015387 ,  1.74481176, -0.7612069 ],\n",
       "       [ 0.3190391 , -0.24937038,  1.46210794, -2.06014071, -0.3224172 ,\n",
       "        -0.38405435,  1.13376944, -1.09989127],\n",
       "       [-0.17242821, -0.87785842,  0.04221375,  0.58281521, -1.10061918,\n",
       "         1.14472371,  0.90159072,  0.50249434],\n",
       "       [ 0.90085595, -0.68372786, -0.12289023, -0.93576943, -0.26788808,\n",
       "         0.53035547, -0.69166075, -0.39675353],\n",
       "       [-0.6871727 , -0.84520564, -0.67124613, -0.0126646 , -1.11731035,\n",
       "         0.2344157 ,  1.65980218,  0.74204416],\n",
       "       [-0.19183555, -0.88762896, -0.74715829,  1.6924546 ,  0.05080775,\n",
       "        -0.63699565,  0.19091548,  2.10025514],\n",
       "       [ 0.12015895,  0.61720311,  0.30017032, -0.35224985, -1.1425182 ,\n",
       "        -0.34934272, -0.20889423,  0.58662319],\n",
       "       [ 0.83898341,  0.93110208,  0.28558733,  0.88514116, -0.75439794,\n",
       "         1.25286816,  0.51292982, -0.29809284],\n",
       "       [ 0.48851815, -0.07557171,  1.13162939,  1.51981682,  2.18557541,\n",
       "        -1.39649634, -1.44411381, -0.50446586],\n",
       "       [ 0.16003707,  0.87616892,  0.31563495, -2.02220122, -0.30620401,\n",
       "         0.82797464,  0.23009474,  0.76201118]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(0,)\n",
      "(1,)\n",
      "(2,)\n",
      "(3,)\n",
      "(4,)\n",
      "(5,)\n",
      "(6,)\n",
      "(7,)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(0, 4)\n",
      "(0, 5)\n",
      "(0, 6)\n",
      "(0, 7)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(1, 4)\n",
      "(1, 5)\n",
      "(1, 6)\n",
      "(1, 7)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(2, 5)\n",
      "(2, 6)\n",
      "(2, 7)\n",
      "(3, 4)\n",
      "(3, 5)\n",
      "(3, 6)\n",
      "(3, 7)\n",
      "(4, 5)\n",
      "(4, 6)\n",
      "(4, 7)\n",
      "(5, 6)\n",
      "(5, 7)\n",
      "(6, 7)\n",
      "(0, 1, 2)\n",
      "(0, 1, 3)\n",
      "(0, 1, 4)\n",
      "(0, 1, 5)\n",
      "(0, 1, 6)\n",
      "(0, 1, 7)\n",
      "(0, 2, 3)\n",
      "(0, 2, 4)\n",
      "(0, 2, 5)\n",
      "(0, 2, 6)\n",
      "(0, 2, 7)\n",
      "(0, 3, 4)\n",
      "(0, 3, 5)\n",
      "(0, 3, 6)\n",
      "(0, 3, 7)\n",
      "(0, 4, 5)\n",
      "(0, 4, 6)\n",
      "(0, 4, 7)\n",
      "(0, 5, 6)\n",
      "(0, 5, 7)\n",
      "(0, 6, 7)\n",
      "(1, 2, 3)\n",
      "(1, 2, 4)\n",
      "(1, 2, 5)\n",
      "(1, 2, 6)\n",
      "(1, 2, 7)\n",
      "(1, 3, 4)\n",
      "(1, 3, 5)\n",
      "(1, 3, 6)\n",
      "(1, 3, 7)\n",
      "(1, 4, 5)\n",
      "(1, 4, 6)\n",
      "(1, 4, 7)\n",
      "(1, 5, 6)\n",
      "(1, 5, 7)\n",
      "(1, 6, 7)\n",
      "(2, 3, 4)\n",
      "(2, 3, 5)\n",
      "(2, 3, 6)\n",
      "(2, 3, 7)\n",
      "(2, 4, 5)\n",
      "(2, 4, 6)\n",
      "(2, 4, 7)\n",
      "(2, 5, 6)\n",
      "(2, 5, 7)\n",
      "(2, 6, 7)\n",
      "(3, 4, 5)\n",
      "(3, 4, 6)\n",
      "(3, 4, 7)\n",
      "(3, 5, 6)\n",
      "(3, 5, 7)\n",
      "(3, 6, 7)\n",
      "(4, 5, 6)\n",
      "(4, 5, 7)\n",
      "(4, 6, 7)\n",
      "(5, 6, 7)\n",
      "(0, 1, 2, 3)\n",
      "(0, 1, 2, 4)\n",
      "(0, 1, 2, 5)\n",
      "(0, 1, 2, 6)\n",
      "(0, 1, 2, 7)\n",
      "(0, 1, 3, 4)\n",
      "(0, 1, 3, 5)\n",
      "(0, 1, 3, 6)\n",
      "(0, 1, 3, 7)\n",
      "(0, 1, 4, 5)\n",
      "(0, 1, 4, 6)\n",
      "(0, 1, 4, 7)\n",
      "(0, 1, 5, 6)\n",
      "(0, 1, 5, 7)\n",
      "(0, 1, 6, 7)\n",
      "(0, 2, 3, 4)\n",
      "(0, 2, 3, 5)\n",
      "(0, 2, 3, 6)\n",
      "(0, 2, 3, 7)\n",
      "(0, 2, 4, 5)\n",
      "(0, 2, 4, 6)\n",
      "(0, 2, 4, 7)\n",
      "(0, 2, 5, 6)\n",
      "(0, 2, 5, 7)\n",
      "(0, 2, 6, 7)\n",
      "(0, 3, 4, 5)\n",
      "(0, 3, 4, 6)\n",
      "(0, 3, 4, 7)\n",
      "(0, 3, 5, 6)\n",
      "(0, 3, 5, 7)\n",
      "(0, 3, 6, 7)\n",
      "(0, 4, 5, 6)\n",
      "(0, 4, 5, 7)\n",
      "(0, 4, 6, 7)\n",
      "(0, 5, 6, 7)\n",
      "(1, 2, 3, 4)\n",
      "(1, 2, 3, 5)\n",
      "(1, 2, 3, 6)\n",
      "(1, 2, 3, 7)\n",
      "(1, 2, 4, 5)\n",
      "(1, 2, 4, 6)\n",
      "(1, 2, 4, 7)\n",
      "(1, 2, 5, 6)\n",
      "(1, 2, 5, 7)\n",
      "(1, 2, 6, 7)\n",
      "(1, 3, 4, 5)\n",
      "(1, 3, 4, 6)\n",
      "(1, 3, 4, 7)\n",
      "(1, 3, 5, 6)\n",
      "(1, 3, 5, 7)\n",
      "(1, 3, 6, 7)\n",
      "(1, 4, 5, 6)\n",
      "(1, 4, 5, 7)\n",
      "(1, 4, 6, 7)\n",
      "(1, 5, 6, 7)\n",
      "(2, 3, 4, 5)\n",
      "(2, 3, 4, 6)\n",
      "(2, 3, 4, 7)\n",
      "(2, 3, 5, 6)\n",
      "(2, 3, 5, 7)\n",
      "(2, 3, 6, 7)\n",
      "(2, 4, 5, 6)\n",
      "(2, 4, 5, 7)\n",
      "(2, 4, 6, 7)\n",
      "(2, 5, 6, 7)\n",
      "(3, 4, 5, 6)\n",
      "(3, 4, 5, 7)\n",
      "(3, 4, 6, 7)\n",
      "(3, 5, 6, 7)\n",
      "(4, 5, 6, 7)\n",
      "(0, 1, 2, 3, 4)\n",
      "(0, 1, 2, 3, 5)\n",
      "(0, 1, 2, 3, 6)\n",
      "(0, 1, 2, 3, 7)\n",
      "(0, 1, 2, 4, 5)\n",
      "(0, 1, 2, 4, 6)\n",
      "(0, 1, 2, 4, 7)\n",
      "(0, 1, 2, 5, 6)\n",
      "(0, 1, 2, 5, 7)\n",
      "(0, 1, 2, 6, 7)\n",
      "(0, 1, 3, 4, 5)\n",
      "(0, 1, 3, 4, 6)\n",
      "(0, 1, 3, 4, 7)\n",
      "(0, 1, 3, 5, 6)\n",
      "(0, 1, 3, 5, 7)\n",
      "(0, 1, 3, 6, 7)\n",
      "(0, 1, 4, 5, 6)\n",
      "(0, 1, 4, 5, 7)\n",
      "(0, 1, 4, 6, 7)\n",
      "(0, 1, 5, 6, 7)\n",
      "(0, 2, 3, 4, 5)\n",
      "(0, 2, 3, 4, 6)\n",
      "(0, 2, 3, 4, 7)\n",
      "(0, 2, 3, 5, 6)\n",
      "(0, 2, 3, 5, 7)\n",
      "(0, 2, 3, 6, 7)\n",
      "(0, 2, 4, 5, 6)\n",
      "(0, 2, 4, 5, 7)\n",
      "(0, 2, 4, 6, 7)\n",
      "(0, 2, 5, 6, 7)\n",
      "(0, 3, 4, 5, 6)\n",
      "(0, 3, 4, 5, 7)\n",
      "(0, 3, 4, 6, 7)\n",
      "(0, 3, 5, 6, 7)\n",
      "(0, 4, 5, 6, 7)\n",
      "(1, 2, 3, 4, 5)\n",
      "(1, 2, 3, 4, 6)\n",
      "(1, 2, 3, 4, 7)\n",
      "(1, 2, 3, 5, 6)\n",
      "(1, 2, 3, 5, 7)\n",
      "(1, 2, 3, 6, 7)\n",
      "(1, 2, 4, 5, 6)\n",
      "(1, 2, 4, 5, 7)\n",
      "(1, 2, 4, 6, 7)\n",
      "(1, 2, 5, 6, 7)\n",
      "(1, 3, 4, 5, 6)\n",
      "(1, 3, 4, 5, 7)\n",
      "(1, 3, 4, 6, 7)\n",
      "(1, 3, 5, 6, 7)\n",
      "(1, 4, 5, 6, 7)\n",
      "(2, 3, 4, 5, 6)\n",
      "(2, 3, 4, 5, 7)\n",
      "(2, 3, 4, 6, 7)\n",
      "(2, 3, 5, 6, 7)\n",
      "(2, 4, 5, 6, 7)\n",
      "(3, 4, 5, 6, 7)\n",
      "(0, 1, 2, 3, 4, 5)\n",
      "(0, 1, 2, 3, 4, 6)\n",
      "(0, 1, 2, 3, 4, 7)\n",
      "(0, 1, 2, 3, 5, 6)\n",
      "(0, 1, 2, 3, 5, 7)\n",
      "(0, 1, 2, 3, 6, 7)\n",
      "(0, 1, 2, 4, 5, 6)\n",
      "(0, 1, 2, 4, 5, 7)\n",
      "(0, 1, 2, 4, 6, 7)\n",
      "(0, 1, 2, 5, 6, 7)\n",
      "(0, 1, 3, 4, 5, 6)\n",
      "(0, 1, 3, 4, 5, 7)\n",
      "(0, 1, 3, 4, 6, 7)\n",
      "(0, 1, 3, 5, 6, 7)\n",
      "(0, 1, 4, 5, 6, 7)\n",
      "(0, 2, 3, 4, 5, 6)\n",
      "(0, 2, 3, 4, 5, 7)\n",
      "(0, 2, 3, 4, 6, 7)\n",
      "(0, 2, 3, 5, 6, 7)\n",
      "(0, 2, 4, 5, 6, 7)\n",
      "(0, 3, 4, 5, 6, 7)\n",
      "(1, 2, 3, 4, 5, 6)\n",
      "(1, 2, 3, 4, 5, 7)\n",
      "(1, 2, 3, 4, 6, 7)\n",
      "(1, 2, 3, 5, 6, 7)\n",
      "(1, 2, 4, 5, 6, 7)\n",
      "(1, 3, 4, 5, 6, 7)\n",
      "(2, 3, 4, 5, 6, 7)\n",
      "(0, 1, 2, 3, 4, 5, 6)\n",
      "(0, 1, 2, 3, 4, 5, 7)\n",
      "(0, 1, 2, 3, 4, 6, 7)\n",
      "(0, 1, 2, 3, 5, 6, 7)\n",
      "(0, 1, 2, 4, 5, 6, 7)\n",
      "(0, 1, 3, 4, 5, 6, 7)\n",
      "(0, 2, 3, 4, 5, 6, 7)\n",
      "(1, 2, 3, 4, 5, 6, 7)\n",
      "(0, 1, 2, 3, 4, 5, 6, 7)\n"
     ]
    }
   ],
   "source": [
    "V = shap_adaptive(f, x)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
